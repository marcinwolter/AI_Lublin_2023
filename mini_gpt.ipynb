{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/AI_Lublin_2023/blob/main/mini_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yir8sk6TVAZS"
      },
      "source": [
        "# GPT text generation from scratch with KerasNLP\n",
        "\n",
        "**Author:** [Jesse Chan](https://github.com/jessechancy)<br>\n",
        "**Date created:** 2022/07/25<br>\n",
        "**Last modified:** 2022/07/25<br>\n",
        "**Description:** Using KerasNLP to train a mini-GPT model for text generation.\n",
        "\n",
        "Modified by M. Wolter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnl_JfkAVAZV"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we will use KerasNLP to build a scaled down Generative\n",
        "Pre-Trained (GPT) model. GPT is a Transformer-based model that allows you to generate\n",
        "sophisticated text from a prompt.\n",
        "\n",
        "We will train the model on the part of [Polish Wikipedia](https://pl.wikipedia.org/) corpus,\n",
        "which allows our GPT model to communicate in Polish. \n",
        "\n",
        "This example combines concepts from\n",
        "[Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
        "with KerasNLP package abstractions. We will demonstrate how KerasNLP tokenization, layers and\n",
        "metrics simplify the training\n",
        "process, and then show how to generate output text using the KerasNLP sampling utilities.\n",
        "\n",
        "Note: If you are running this example on a Colab,\n",
        "make sure to enable GPU runtime for faster training.\n",
        "\n",
        "This example requires KerasNLP. You can install it via the following command:\n",
        "`pip install keras-nlp`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl4zUCyIVAZV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install keras-nlp"
      ],
      "metadata": {
        "id": "MGhagHHPqPOh",
        "outputId": "0d3f531e-37a6-476e-b867-a2f4bea97cdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (23.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (2.12.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (2.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gi7UievAVAZW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W4AhairVAZX"
      },
      "source": [
        "## Settings & hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "68M69NFIVAZX"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LEN = 128\n",
        "MIN_TRAINING_SEQ_LEN = 450\n",
        "\n",
        "# Model\n",
        "EMBED_DIM = 256\n",
        "FEED_FORWARD_DIM = 256\n",
        "NUM_HEADS = 3\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = 5000  # Limits parameters in model.\n",
        "\n",
        "# Training\n",
        "EPOCHS = 6\n",
        "\n",
        "# Inference\n",
        "NUM_TOKENS_TO_GENERATE = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose the language\n",
        "\n",
        "GPT can be trained using:\n",
        "* \"English\" - SimpleBooks dataset\n",
        "* \"Polish\"  - part of Polish Wikipedia"
      ],
      "metadata": {
        "id": "omuuv-hv6yZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lang = \"English\"   # \"Polish\""
      ],
      "metadata": {
        "id": "mmuF4MxF6sNw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wiki-dump-reader allows us to load the wikipedia and convert articles into plain text."
      ],
      "metadata": {
        "id": "5WUCRtXOEaND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wiki-dump-reader"
      ],
      "metadata": {
        "id": "Qac0QD0uSBgG",
        "outputId": "b95f1b29-c8e5-4a7e-ba5c-65c6188ae3ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wiki-dump-reader in /usr/local/lib/python3.10/dist-packages (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wiki_dump_reader import Cleaner, iterate\n",
        "\n",
        "text_data = open(\"plwiki.txt\",'w')\n",
        "cleaner = Cleaner()\n",
        "for title, text in iterate('plwiki-20220820-pages-articles1.xml-p1p187037'):\n",
        "    text = cleaner.clean_text(text)\n",
        "    cleaned_text, links = cleaner.build_links(text)\n",
        "    text_data.write(cleaned_text)\n",
        "   \n",
        "text_data.close() "
      ],
      "metadata": {
        "id": "Yh2UisfNQzYH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load wikipedia from plwiki.txt file and filter out short lines.\n",
        "Now, let's download the dataset! The Polish Wikipedia is huge, so we load just only one dataset with articles.\n"
      ],
      "metadata": {
        "id": "QymDhjVIE6i3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t5DIMDHAVAZY"
      },
      "outputs": [],
      "source": [
        "if lang==\"Polish\":\n",
        "\n",
        "  ! wget https://dumps.wikimedia.your.org/plwiki/20220820/plwiki-20220820-pages-articles1.xml-p1p187037.bz2 \n",
        "  ! bzip2 -d plwiki-20220820-pages-articles1.xml-p1p187037.bz2\n",
        "\n",
        "  # Load wikipedia train set - 98 500 lines and filter out short lines.\n",
        "  raw_train_ds = (\n",
        "  #    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
        "    tf.data.TextLineDataset(\"plwiki.txt\").skip(1500).take(100000)\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=256)\n",
        "    )\n",
        "\n",
        "# Load wikipedia validation set of 1500 lines and filter out short lines.\n",
        "raw_val_ds = (\n",
        "#    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
        "    tf.data.TextLineDataset(\"plwiki.txt\").take(1500)\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td90gCoS4qpM"
      },
      "source": [
        "## Load the data - English SimpleBooks\n",
        "\n",
        "Now, let's download the dataset! The SimpleBooks dataset consists of 1,573 Gutenberg books, and has\n",
        "one of the smallest vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k,\n",
        "a third of WikiText-103's, with around the same number of tokens (~100M). This makes it easy to fit a small model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5zBGl_w94qpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69477806-8661-4f9b-b778-624e85357ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n",
            "282386239/282386239 [==============================] - 10s 0us/step\n"
          ]
        }
      ],
      "source": [
        "if lang==\"English\":\n",
        "\n",
        "  keras.utils.get_file(\n",
        "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
        "    extract=True,\n",
        "  )\n",
        "  dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
        "\n",
        "  # Load simplebooks-92 train set and filter out short lines.\n",
        "  raw_train_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=256)\n",
        "  )\n",
        "\n",
        "  # Load simplebooks-92 validation set and filter out short lines.\n",
        "  raw_val_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rivoGs7OVAZY"
      },
      "source": [
        "## Train the tokenizer\n",
        "\n",
        "We train the tokenizer from the training dataset for a vocabulary size of `VOCAB_SIZE`,\n",
        "which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, as\n",
        "we will see later on\n",
        "that it has a large affect on the number of model parameters. We also don't want to include\n",
        "*too few* vocabulary terms, or there would be too many out-of-vocabulary (OOV) sub-words. In\n",
        "addition, three tokens are reserved in the vocabulary:\n",
        "\n",
        "- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both\n",
        "`reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider\n",
        "`0`/`vocab[0]` as the default padding.\n",
        "- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n",
        "`WordPieceTokenizer`.\n",
        "- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token\n",
        "representing the beginning of each line of training data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Warning: the training takes some time!!!**"
      ],
      "metadata": {
        "id": "F8HdBGzEFTv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KOwXQJE1VAZZ"
      },
      "outputs": [],
      "source": [
        "# Train tokenizer vocabulary\n",
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    raw_train_ds,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    lowercase=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEwsGTQ4VAZZ"
      },
      "source": [
        "## Load tokenizer\n",
        "\n",
        "We use the vocabulary data to initialize\n",
        "`keras_nlp.tokenizers.WordPieceTokenizer`. WordPieceTokenizer is an efficient\n",
        "implementation of the WordPiece algorithm used by BERT and other models. It will strip,\n",
        "lower-case and do other irreversible preprocessing operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GZjsw1nOVAZa"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    lowercase=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xls-py_yVAZa"
      },
      "source": [
        "## Tokenize data\n",
        "\n",
        "We preprocess the dataset by tokenizing and splitting it into `features` and `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "n_dtOlxhVAZa"
      },
      "outputs": [],
      "source": [
        "# packer adds a start token\n",
        "start_packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    outputs = tokenizer(inputs)\n",
        "    features = start_packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "# Tokenize and split into train and label sequences.\n",
        "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rm_sRk6VAZa"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "We create our scaled down GPT model with the following layers:\n",
        "\n",
        "- One `keras_nlp.layers.TokenAndPositionEmbedding` layer, which combines the embedding\n",
        "for the token and its position.\n",
        "- Multiple `keras_nlp.layers.TransformerDecoder` layers, with the default causal masking.\n",
        "The layer has no cross-attention when run with decoder sequence only.\n",
        "- One final dense linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JEfY0e3gVAZb"
      },
      "outputs": [],
      "source": [
        "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "# Embedding.\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "# Transformer decoders.\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
        "# Output.\n",
        "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mw1-3hnVAZb"
      },
      "source": [
        "Let's take a look at our model summary - a large majority of the\n",
        "parameters are in the `token_and_position_embedding` and the output `dense` layer!\n",
        "This means that the vocabulary size (`VOCAB_SIZE`) has a large affect on the size of the model,\n",
        "while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpXzXlx4VAZb",
        "outputId": "ecf56576-f7c0-48fb-ba4f-24dd36cfea02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, None, 256)        1312768   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_decoder (Transf  (None, None, 256)        394749    \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            " transformer_decoder_1 (Tran  (None, None, 256)        394749    \n",
            " sformerDecoder)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 5000)        1285000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,387,266\n",
            "Trainable params: 3,387,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMdvEttSVAZb"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that we have our model, let's train it with the `fit()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgYi7PATVAZc",
        "outputId": "64ef8569-784b-4718-82ad-aea5d30565fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "3169/3169 - 273s - loss: 4.5675 - perplexity: 96.6795 - val_loss: 4.1558 - val_perplexity: 64.4383 - 273s/epoch - 86ms/step\n",
            "Epoch 2/6\n",
            "3169/3169 - 120s - loss: 4.0591 - perplexity: 58.1489 - val_loss: 3.9813 - val_perplexity: 53.9634 - 120s/epoch - 38ms/step\n",
            "Epoch 3/6\n",
            "3169/3169 - 119s - loss: 3.9445 - perplexity: 51.8465 - val_loss: 3.9290 - val_perplexity: 51.3033 - 119s/epoch - 38ms/step\n",
            "Epoch 4/6\n",
            "3169/3169 - 118s - loss: 3.8827 - perplexity: 48.7388 - val_loss: 3.8987 - val_perplexity: 49.8520 - 118s/epoch - 37ms/step\n",
            "Epoch 5/6\n",
            "3169/3169 - 120s - loss: 3.8393 - perplexity: 46.6682 - val_loss: 3.8494 - val_perplexity: 47.3274 - 120s/epoch - 38ms/step\n",
            "Epoch 6/6\n",
            "3169/3169 - 120s - loss: 3.8084 - perplexity: 45.2459 - val_loss: 3.8537 - val_perplexity: 47.5800 - 120s/epoch - 38ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a8426a230>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXjfaX20VAZc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "With our trained model, we can test it out to gauge it's performance. To do this\n",
        "we can seed our model with an input sequence starting with the `\"[BOS]\"` token,\n",
        "and progressively sample the model by making predictions for each subsequent\n",
        "token in a loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06g5Zu3gVAZc"
      },
      "source": [
        "We will use the `keras_nlp.samplers` module for inference, which requires a\n",
        "callback function wrapping the model we just trained. This wrapper calls\n",
        "the model and returns the logit predictions for the current token we are\n",
        "generating.\n",
        "\n",
        "Note: There are two pieces of more advanced functionality available when\n",
        "defining your callback. The first is the ability to take in a `cache` of states\n",
        "computed in previous generation steps, which can be used to speed up generation.\n",
        "The second is the ability to output the final dense \"hidden state\" of each\n",
        "generated token. This is used by `keras_nlp.samplers.ContrastiveSampler`, which\n",
        "avoids repetition by penalizing repeated hidden states. Both are optional, and\n",
        "we will ignore them for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jy2AJinQVAZc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def next(prompt, cache, index):\n",
        "    logits = model(prompt)[:, index - 1, :]\n",
        "    # Ignore hidden states for now; only needed for contrastive search.\n",
        "    hidden_states = None\n",
        "    return logits, hidden_states, cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Hmkj-YVAZd"
      },
      "source": [
        "Creating the wrapper function is the most complex part of using these functions. Now that\n",
        "it's done, let's test out the different utilties, starting with greedy search."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def toPolish(txt):\n",
        "  #txt = str(txt)\n",
        "  \n",
        "  #print(txt.__repr__())\n",
        "  \n",
        "  txt = txt.replace(\"tf.Tensor([b'[BOS] \",\"\")\n",
        "  txt = txt.replace(\"'], shape=(1,), dtype=string)\",\"\")\n",
        "  \n",
        "  txt = txt.replace('\\\\xe2\\\\x80\\\\x93', '-')\n",
        "  txt = txt.replace('\\\\xc5\\\\x82', 'ł')   \n",
        "  txt = txt.replace('\\\\xc4\\\\x99', 'ę') \n",
        "  txt = txt.replace('\\\\xc4\\\\x85', 'ą')\n",
        "  txt = txt.replace('\\\\xc5\\\\x9b', 'ś')\n",
        "  txt = txt.replace('\\\\xc3\\\\xb3', 'ó')\n",
        "  txt = txt.replace('\\\\xc5\\\\xbc', 'ż')\n",
        "  txt = txt.replace('\\\\xc4\\\\x87', 'ć')\n",
        "  txt = txt.replace('\\\\xc5\\\\xbc', 'ż')\n",
        "  txt = txt.replace('\\\\xc5\\\\x84', 'ń')\n",
        "  txt = txt.replace(\"\\\\'\", '')\n",
        "  txt = txt.replace('\"', '')\n",
        "  #print(txt.__repr__())\n",
        "  #txt = txt.encode('iso-8859-1').decode('utf-8')\n",
        "  return txt"
      ],
      "metadata": {
        "id": "9eihD_ZITh6a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qv_prGnVAZd"
      },
      "source": [
        "### Top-K search\n",
        "\n",
        "We sample the next token from the probability distribution\n",
        "provided by the model. We select out the top `k` most\n",
        "probable tokens, and distribute the probabiltiy mass over them before sampling. This way,\n",
        "we won't be sampling from low probability tokens, and hence we would have less\n",
        "nonsensical words!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW65DdcoVAZd",
        "outputId": "c3728619-1dfa-4c2f-9301-88a8dac90cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Insert prompt (q breaks):\n",
            "\n",
            "Generated text: \n",
            " the king of the king , with his king , was king of the court who was , as he was , and was to argue with the king , who was to be the king , and king arthur and king arthur , and sir tristram was very pleased indeed , for sir launcelot of the court , and sir percival was very great and very great and intent to the king of the court , and sir tristram perceived that sir launcelot had very great joy and beheld that he had beheld that sir launcelot of the lake wherefore he beheld that he was nigh unto sir tristram : so sir tristram sir tristram was :  sir\n",
            "\n",
            "Insert prompt (q breaks):\n",
            "my excursion to London was frustrating\n",
            "Generated text: \n",
            "my excursion to London was frustrating  you know , i  m not ,  said the man .  he is a very good fellow in the world to be very well , but i think that i have never been able to see him . he is a very fine fellow , for he is not a great deal better than he has a good deal of money . he is so bad , and he is a little fellow , for the fellow has a good deal of time and a good deal of money , and i think i have to be able to make him feel as though it was wrong . he says he is a great deal better , and he has a lot of\n",
            "\n",
            "Insert prompt (q breaks):\n",
            "q\n"
          ]
        }
      ],
      "source": [
        "\n",
        "while True:\n",
        "  print(\"Insert prompt (q breaks):\")\n",
        "  prompt = input()\n",
        "  if prompt=='q':\n",
        "    break\n",
        "\n",
        "  prompt_tokens = start_packer(tokenizer([prompt]))\n",
        "\n",
        "  sampler = keras_nlp.samplers.TopKSampler(k=4) #GreedySampler()\n",
        "  output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,  # Start sampling immediately after the [BOS] token.\n",
        "  )\n",
        "  txt = tokenizer.detokenize(output_tokens)\n",
        "\n",
        "  txt_str = toPolish(str(txt))\n",
        "  print(f\"Generated text: \\n{prompt} {txt_str}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLcjAuZ-VAZd"
      },
      "source": [
        "As you can see, the short training doesn't allow to generate sensible text. But this micro GPT doesn't produce a complete nonsense neither. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVruuc6-VAZm"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "To recap, in this example, we use KerasNLP layers to train a sub-word vocabulary,\n",
        "tokenize training data, create a miniature GPT model, and perform inference with the\n",
        "text generation library.\n",
        "\n",
        "If you would like to understand how Transformers work, or learn more about training the\n",
        "full GPT model, here are some further readings:\n",
        "\n",
        "- Attention Is All You Need [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
        "- GPT-3 Paper [Brown et al., 2020](https://arxiv.org/abs/2005.14165)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}